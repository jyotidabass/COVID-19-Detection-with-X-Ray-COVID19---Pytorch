{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![Star Wars Ep.IV New Hope](https://i.stack.imgur.com/jmKDh.jpg)\n# Hyperdrive into PyTorch - Tutorial (Dive into Computer Vision in Health Care)\nYeah, as you guessed I am a huge fan of **Star Wars**. Maybe, I learn Artificial Intelligence and Deep Learning to build my own C-3PO like the young Anakin Skywalker did but I would build a Bot for Health Care to help Humans in saving the Humanity. As it is the need of the time in this dreadful pandemic. Ok, lets consider building an Bot like see in Star Wars, ability to process what it sees will be our first priority right? Else our C-3PO will be blind üòÇ. \n\nComputer Vision is a field of artificial intelligence that enables machines the ability to \"See\" things. The word \"See\" is not what you think of it as just what a camera do, capturing moments. But thats not it, \"See\" means that machine can able to perceive things and understands the difference or relation between the real world physical entities. This state of \"See\" is we achieved now takes a lot of hard word, research and iterations. We achieved it by solving little pieces of it like Image Classification, Image Segmentation and Object Segmentation, etc. \n\n> We are so blessed to be born in this period, as we have a plethora of resesources and community of people to guide us through.\n\n### Why AI in Health Care is crucial?\nThough many fields can be benificial by the implementation of Artificial Intelligence, it is the field of Health Care which would be benifitted the most and in turn Humanity. It very hard to diagnose X-ray scans, and take decisions deterministically at the right momemt. There is also human errors associated with it. Computers can contribute a ton to Health and Medicine as an \"SideKick\" to humans(Doctors). I wouldn't say Computer can outperform with absolute percision, it may have fail in some cases. That's why we have computers to assist Doctors.\n\n![Pneumoia Detection](https://i.imgur.com/jZqpV51.png)\n\n<font color=\"red\">If you find this notebook helpful please do upvote this notebook. It would make me smile and motivate me.</font>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"Since you already starting reading **Kaggle Notebooks**, I assume you may have come across this Deep Learning library that is familiar with most Data Scientists and AI Researchers. \n<img src=\"https://www.quintagroup.com/blog/blog-images/machine-learning-libraries/pytorch.png/@@images/cce95fa7-4048-42c1-b768-dca2a747b8bd.png\" width=300 alt=\"PyTorch\" >\n\n## PyTorch\n\nPyTorch is a Deep Learning library which makes Tensor Computations, a core part in Deep Learning applications. It is developed and used by Facebook AI Research Lab. It has been library which attracted many researchers and develop many State-of-the-Art Algorithms across all the fields of Deep Learning such as Computer Vision, NLP and Tabular Data Processing. \n\n### Why learn PyTorch?\nPyTorch is home for many powerful projects and future researches in Deep Learning and Artificial Intelligence. FaceBook AI Research (FAIR) contributed an enormous amount of new mind boggling researches and projects built o PyTorch. Detectron2(CV), HugginngFace(NLP),DETR(CV) are some of amazing projects by them based on PyTorch. So you will be able to see and develop your own State-of-the-Art applications inspiring from these projects. And also PyTorch is so friendly looking with most of the code will look like vanila \"Python\" Structures.\n> I learnt PyTorch after I got comfortable with Tensorflow/Keras. I think PyTorch has more flexibility for a coder and Customizable. I get to know the process behind the Traning of Model better with PyTorch.\n\n## Let's get started\nOk, I can hear you say \"ENOUGH\". Alright, let's get started in this notebook I am gonna guide you through a image classification task using PyTorch. So the problem statement that I am gonna take will be...","metadata":{}},{"cell_type":"markdown","source":"# COVID-19 Detection using Chest Xray\nAs you probably know, we are in the tight hold of a pandemic which has its disastarous effect on this world. We are standing together even while physically distancing ourselves, protecting others and caring for others in this dreadful times. Humanity still deserved to live for million years. But we should contribute our role in this fight against an invisible enemy. \n\n> Many Data Scientist are working getting predictions and developing models for strategies. We can assist HealthCare workers and Doctors by developing a AI Assistment or Model. A Image Classification model can handle a lot of worries for Doctors, especially in X Ray Diagnosis.\n\n**In this notebook, I am going to train a CNN classifier which can classify COVID-19 Infected Lung Xray images from Healthy Lung Xray images. For that we need dataset from both classes, I used the dataset from two sources,**\n\n- Cohen's [COVID Chest X-ray Dataset](https://github.com/ieee8023/covid-chestxray-dataset) \n- Paul Mooney's [Chest X-ray Dataset (Pneumonia)](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia)\n\n>üòÉ I am very grateful and thankful to the huge efforts by the Open-Source Community for collecting data in a repository. This project would be possible without those repositories.\n\nI create the aggregate the images from those two repositories and get them ready for training. Check out the code.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nimport matplotlib.pyplot as plt\nimport time\nimport copy\nfrom random import shuffle\n\nimport tqdm.notebook as tqdm\n\nimport sklearn\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nfrom sklearn.metrics import classification_report\nfrom PIL import Image\nimport cv2\n\nimport os\nimport shutil","metadata":{"id":"e_eezmSZJJe7","execution":{"iopub.status.busy":"2021-10-07T06:05:46.747285Z","iopub.execute_input":"2021-10-07T06:05:46.748335Z","iopub.status.idle":"2021-10-07T06:05:49.544704Z","shell.execute_reply.started":"2021-10-07T06:05:46.748277Z","shell.execute_reply":"2021-10-07T06:05:49.543695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ieee8023/covid-chestxray-dataset.git","metadata":{"id":"mxYL0SvWMTRF","outputId":"07b17da0-c8c9-4e3f-ba2f-d81e19c10d9e","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-07T06:05:49.547253Z","iopub.execute_input":"2021-10-07T06:05:49.547687Z","iopub.status.idle":"2021-10-07T06:06:17.013102Z","shell.execute_reply.started":"2021-10-07T06:05:49.547644Z","shell.execute_reply":"2021-10-07T06:06:17.011718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('./covid-chestxray-dataset/metadata.csv')\nselected_df = df[df.finding==\"Pneumonia/Viral/COVID-19\"]\nselected_df = selected_df[(selected_df.view == \"AP\") | (selected_df.view == \"PA\")]\nselected_df.head(2)","metadata":{"id":"ZghMYdMHMGuj","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-07T06:06:17.015538Z","iopub.execute_input":"2021-10-07T06:06:17.015972Z","iopub.status.idle":"2021-10-07T06:06:17.087938Z","shell.execute_reply.started":"2021-10-07T06:06:17.015924Z","shell.execute_reply":"2021-10-07T06:06:17.086671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = selected_df.filename.values.tolist()","metadata":{"id":"RADFgzxtKwbj","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-07T06:06:17.089922Z","iopub.execute_input":"2021-10-07T06:06:17.090381Z","iopub.status.idle":"2021-10-07T06:06:17.096776Z","shell.execute_reply.started":"2021-10-07T06:06:17.090337Z","shell.execute_reply":"2021-10-07T06:06:17.095227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs('./COVID19-DATASET/train/covid19')\nos.makedirs('./COVID19-DATASET/train/normal')","metadata":{"id":"yvdZqbdhLQLy","execution":{"iopub.status.busy":"2021-10-07T06:06:17.103359Z","iopub.execute_input":"2021-10-07T06:06:17.10393Z","iopub.status.idle":"2021-10-07T06:06:17.110542Z","shell.execute_reply.started":"2021-10-07T06:06:17.103884Z","shell.execute_reply":"2021-10-07T06:06:17.109171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COVID_PATH = './COVID19-DATASET/train/covid19'\nNORMAL_PATH = './COVID19-DATASET/train/normal'","metadata":{"id":"2yLVFTOzMBc4","execution":{"iopub.status.busy":"2021-10-07T06:06:17.117259Z","iopub.execute_input":"2021-10-07T06:06:17.117878Z","iopub.status.idle":"2021-10-07T06:06:17.122952Z","shell.execute_reply.started":"2021-10-07T06:06:17.117831Z","shell.execute_reply":"2021-10-07T06:06:17.121127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image in images:\n    shutil.copy(os.path.join('./covid-chestxray-dataset/images', image), os.path.join(COVID_PATH, image))","metadata":{"id":"zN4DENFlLW9U","execution":{"iopub.status.busy":"2021-10-07T06:06:17.125259Z","iopub.execute_input":"2021-10-07T06:06:17.125993Z","iopub.status.idle":"2021-10-07T06:06:17.425486Z","shell.execute_reply.started":"2021-10-07T06:06:17.125934Z","shell.execute_reply":"2021-10-07T06:06:17.423987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image in os.listdir('../input/chest-xray-pneumonia/chest_xray/train/NORMAL')[:300]:\n    shutil.copy(os.path.join('../input/chest-xray-pneumonia/chest_xray/train/NORMAL', image), os.path.join(NORMAL_PATH, image))","metadata":{"id":"FP1C7Ai0QEnw","execution":{"iopub.status.busy":"2021-10-07T06:06:17.427617Z","iopub.execute_input":"2021-10-07T06:06:17.428082Z","iopub.status.idle":"2021-10-07T06:06:23.443849Z","shell.execute_reply.started":"2021-10-07T06:06:17.428034Z","shell.execute_reply":"2021-10-07T06:06:23.442799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = './COVID19-DATASET/train'","metadata":{"id":"GG1zTPHaRSKS","execution":{"iopub.status.busy":"2021-10-07T06:06:23.445877Z","iopub.execute_input":"2021-10-07T06:06:23.446291Z","iopub.status.idle":"2021-10-07T06:06:23.45215Z","shell.execute_reply.started":"2021-10-07T06:06:23.446246Z","shell.execute_reply":"2021-10-07T06:06:23.450886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = os.listdir(DATA_PATH)\nimage_count = {}\nfor i in class_names:\n    image_count[i] = len(os.listdir(os.path.join(DATA_PATH,i)))\n\n#Plotting Distribution of Each Classes\nfig1, ax1 = plt.subplots()\nax1.pie(image_count.values(),\n        labels = image_count.keys(),\n        shadow=True,\n        autopct = '%1.1f%%',\n        startangle=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T06:06:23.454321Z","iopub.execute_input":"2021-10-07T06:06:23.455146Z","iopub.status.idle":"2021-10-07T06:06:23.558061Z","shell.execute_reply.started":"2021-10-07T06:06:23.455069Z","shell.execute_reply":"2021-10-07T06:06:23.556699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have taken correct propositions of data from each classes while staging my data to avoid overfitted results. Medical Image Datasets will always be like this, we don't have enough data on victim rather we have so many healthy ones. That is what we called is a \"Skewed Dataset\", obviously we developed methods to approach those problems (like K-Fold Cross Validation) that will explain that in my next notebook.","metadata":{}},{"cell_type":"markdown","source":"Lets view some images to know what we are dealing with here. Feel free the run the cell to view images at random from the repository.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(16,5))\nfig.suptitle(\"COVID19 Positive\", size=22)\nimg_paths = os.listdir(COVID_PATH)\nshuffle(img_paths)\n\nfor i,image in enumerate(img_paths[:4]):\n    img = cv2.imread(os.path.join(COVID_PATH, image))\n    plt.subplot(1,4, i+1, frameon=False)\n    plt.imshow(img)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T06:06:23.560138Z","iopub.execute_input":"2021-10-07T06:06:23.560624Z","iopub.status.idle":"2021-10-07T06:06:24.835988Z","shell.execute_reply.started":"2021-10-07T06:06:23.560576Z","shell.execute_reply":"2021-10-07T06:06:24.834762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(16,5))\nfig.suptitle(\"COVID19 Negative - Healthy\", size=22)\nimg_paths = os.listdir(NORMAL_PATH)\nshuffle(img_paths)\n\nfor i,image in enumerate(img_paths[:4]):\n    img = cv2.imread(os.path.join(NORMAL_PATH, image))\n    plt.subplot(1,4, i+1, frameon=False)\n    plt.imshow(img)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T06:06:24.837517Z","iopub.execute_input":"2021-10-07T06:06:24.837944Z","iopub.status.idle":"2021-10-07T06:06:26.76661Z","shell.execute_reply.started":"2021-10-07T06:06:24.837889Z","shell.execute_reply":"2021-10-07T06:06:26.765383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Have you seen how similar they look!!. Yeah right, sometimes even for a medical expert. It is hard to diagnose with Xray images. That's why we can help them with our model.","metadata":{}},{"cell_type":"markdown","source":"## Transforms\nData Transforms or Augmentation is the synthesis of new data using the available with some little manipulations and image processing. Augmentation will help generalizing our model, avoids Over-fitting to the training data. Since we have relatively little amounts of data for training and validation we will synthesis some extra data through Image Transforms.\n\n<strong>torchvision</strong> from PyTorch provides,various tools that we can use to perform various tasks for Computer Vision with ease of use.\n\nFew most used tools include,\n- [transforms](https://pytorch.org/docs/stable/torchvision/transforms.html)- Image Data Augementation\n- [datasets](https://pytorch.org/docs/stable/torchvision/datasets.html)- Dataset loading and handling\n- [models](https://pytorch.org/docs/stable/torchvision/models.html)- Deep Learning Pre-Defined SOTA Models\n\nThese tools will be so handy for us, so that we can concentrate on optimizing our results better.","metadata":{}},{"cell_type":"code","source":"#Statistics Based on ImageNet Data for Normalisation\nmean_nums = [0.485, 0.456, 0.406]\nstd_nums = [0.229, 0.224, 0.225]\n\ndata_transforms = {\"train\":transforms.Compose([\n                                transforms.Resize((150,150)), #Resizes all images into same dimension\n                                transforms.RandomRotation(10), # Rotates the images upto Max of 10 Degrees\n                                transforms.RandomHorizontalFlip(p=0.4), #Performs Horizantal Flip over images \n                                transforms.ToTensor(), # Coverts into Tensors\n                                transforms.Normalize(mean = mean_nums, std=std_nums)]), # Normalizes\n                    \"val\": transforms.Compose([\n                                transforms.Resize((150,150)),\n                                transforms.CenterCrop(150), #Performs Crop at Center and resizes it to 150x150\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=mean_nums, std = std_nums)\n                    ])}","metadata":{"id":"AY4KWL6nfxCw","execution":{"iopub.status.busy":"2021-10-07T06:06:26.768423Z","iopub.execute_input":"2021-10-07T06:06:26.769101Z","iopub.status.idle":"2021-10-07T06:06:26.781069Z","shell.execute_reply.started":"2021-10-07T06:06:26.769055Z","shell.execute_reply":"2021-10-07T06:06:26.779572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and Validation Data Split\nWe split our dataset into train and validation sets for training and validating our model with seperate datasets.","metadata":{}},{"cell_type":"code","source":"def load_split_train_test(datadir, valid_size = .2):\n    train_data = datasets.ImageFolder(datadir,       \n                    transform=data_transforms['train']) #Picks up Image Paths from its respective folders and label them\n    test_data = datasets.ImageFolder(datadir,\n                    transform=data_transforms['val'])\n    num_train = len(train_data)\n    indices = list(range(num_train))\n    split = int(np.floor(valid_size * num_train))\n    np.random.shuffle(indices)\n    train_idx, test_idx = indices[split:], indices[:split]\n    dataset_size = {\"train\":len(train_idx), \"val\":len(test_idx)}\n    train_sampler = SubsetRandomSampler(train_idx) # Sampler for splitting train and val images\n    test_sampler = SubsetRandomSampler(test_idx)\n    trainloader = torch.utils.data.DataLoader(train_data,\n                   sampler=train_sampler, batch_size=8) # DataLoader provides data from traininng and validation in batches\n    testloader = torch.utils.data.DataLoader(test_data,\n                   sampler=test_sampler, batch_size=8)\n    return trainloader, testloader, dataset_size\ntrainloader, valloader, dataset_size = load_split_train_test(DATA_PATH, .2)\ndataloaders = {\"train\":trainloader, \"val\":valloader}\ndata_sizes = {x: len(dataloaders[x].sampler) for x in ['train','val']}\nclass_names = trainloader.dataset.classes\nprint(class_names)","metadata":{"id":"b0I-LSUfhsEu","outputId":"cf98846a-6b01-4af6-b600-c9aa532c9d2b","execution":{"iopub.status.busy":"2021-10-07T06:06:26.782919Z","iopub.execute_input":"2021-10-07T06:06:26.783826Z","iopub.status.idle":"2021-10-07T06:06:26.811809Z","shell.execute_reply.started":"2021-10-07T06:06:26.783756Z","shell.execute_reply":"2021-10-07T06:06:26.810374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Like I said, `datasets.ImageFolder` takes the images from each folder named after the class name and automatically labels them. Then `data.DataLoader` loads those labelled images and tracks of the Train Data(Image) and Label(Class Name). Those are the X and Y value which we take for training. ","metadata":{}},{"cell_type":"code","source":"def imshow(inp, size =(30,30), title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = mean_nums\n    std = std_nums\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.figure(figsize=size)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title, size=30)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])","metadata":{"id":"VsnUH-EiirEm","outputId":"61a55c78-b8f3-4116-be13-ef8ba509b692","execution":{"iopub.status.busy":"2021-10-07T06:06:26.813851Z","iopub.execute_input":"2021-10-07T06:06:26.814798Z","iopub.status.idle":"2021-10-07T06:06:28.401657Z","shell.execute_reply.started":"2021-10-07T06:06:26.814752Z","shell.execute_reply":"2021-10-07T06:06:28.400116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ofcourse, we don't want to sit for hours to see our model training. That's why we have GPU with us, now we are going to play game using GPU but this game is *The Imitation Game*\n\nPyTorch has `device` object to load the data into the either of two hardware [CPU or CUDA(GPU)]","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device=torch.device(\"cuda:0\")\n    print(\"Training on GPU... Ready for HyperJump...\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Training on CPU... May the force be with you...\")","metadata":{"id":"oWc_6YWX2bDf","execution":{"iopub.status.busy":"2021-10-07T06:06:28.403353Z","iopub.execute_input":"2021-10-07T06:06:28.403833Z","iopub.status.idle":"2021-10-07T06:06:28.459543Z","shell.execute_reply.started":"2021-10-07T06:06:28.403787Z","shell.execute_reply":"2021-10-07T06:06:28.458175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"F6p9OIFyRhCk","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-07T06:06:28.46109Z","iopub.execute_input":"2021-10-07T06:06:28.461837Z","iopub.status.idle":"2021-10-07T06:06:28.481004Z","shell.execute_reply.started":"2021-10-07T06:06:28.461787Z","shell.execute_reply":"2021-10-07T06:06:28.479895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Definition\nWe reached the fun part !!!. \n\nWe use DenseNet-121 Architecture as the core here for our DL Model. We also use pre-trained weights as a means to Transfer Learning. To learn and achieve higher accuracy on our model faster.\n\nTransfer Learning is a powerful technique that we use in Deep Learning, which is we use the model parameters which are already trained on a larger dataset (ie.ImageNet Dataset consist of 1000 Classes). For instance, its process of transfering the Knowledge of a learnt man into another. Pretrained model will already trained to extract micro features like curves and lines. We will have the previous knowledge to initalize with and we can train the model to look for what it should look.\n\n![DenseNet-121](https://miro.medium.com/max/1400/1*vIZhPImFr9Gjpx6ZB7IOJg.png)\n\nYou can learn more about Densenets here in this [paper](https://arxiv.org/abs/1608.06993).\n\n\nHere I am traning all the the Layers, instead of freezing CNN Layers and training only the Fully Connected Layer (Classification Layer) like most people do in Classification Task. Since we are handling Medical Images here we have to train the CNN layers completely or Partially.","metadata":{}},{"cell_type":"code","source":"def CNN_Model(pretrained=True):\n    model = models.densenet121(pretrained=pretrained) # Returns Defined Densenet model with weights trained on ImageNet\n    num_ftrs = model.classifier.in_features # Get the number of features output from CNN layer\n    model.classifier = nn.Linear(num_ftrs, len(class_names)) # Overwrites the Classifier layer with custom defined layer for transfer learning\n    model = model.to(device) # Transfer the Model to GPU if available\n    return model\n\nmodel = CNN_Model(pretrained=True)\n\n# specify loss function (categorical cross-entropy loss)\ncriterion = nn.CrossEntropyLoss() \n\n# Specify optimizer which performs Gradient Descent\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) # Learning Scheduler","metadata":{"id":"EvSXNkTl2ROh","execution":{"iopub.status.busy":"2021-10-07T06:31:01.41932Z","iopub.execute_input":"2021-10-07T06:31:01.419733Z","iopub.status.idle":"2021-10-07T06:31:01.779932Z","shell.execute_reply.started":"2021-10-07T06:31:01.419694Z","shell.execute_reply":"2021-10-07T06:31:01.77884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we haven't froze the CNN layer parameters untrainable, we are going to train a huge number of parameters.","metadata":{}},{"cell_type":"code","source":"pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"Number of trainable parameters: \\n{}\".format(pytorch_total_params))","metadata":{"id":"y9Jb_lED40mH","outputId":"70c2e152-cd5b-4332-9d2d-f85864ab00c4","execution":{"iopub.status.busy":"2021-10-07T06:06:36.196362Z","iopub.execute_input":"2021-10-07T06:06:36.196832Z","iopub.status.idle":"2021-10-07T06:06:36.211185Z","shell.execute_reply.started":"2021-10-07T06:06:36.19679Z","shell.execute_reply":"2021-10-07T06:06:36.209412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training\nWelcome to PyTorch, here you are free to write your codes as you like with flexibility and comfortability. So shall we start writing our training code?\n\nWe are not using any wrapper class like Keras for Tensorflow, PyTorchLightning for PyTorch, we are going to write the training code ourselves from scratch. We won't regret mastering writing the Trainer code for pytorch, it would help you a ton with theoretical understanding with lavishness. \n\n- `model.train()` and `model.eval()` are called respectively on Training and Evaluation phase.\n- Intialise Loss and Accuracy Variable of the epochs to zero\n- Passing an `dataloader` object to a interative function we get Image Data and Label in batches for every loop\n- Transfer the `Data` and `label` to GPU if available\n- If it's Training Phase, we find Gradients and perform Back Progation for that we write the code under `torch.set_grad_enabled()` to instruct the PyTorch to consider the parameters for Backpropagation updation.\n- Feed in the `Data` (X) to the `model` and it will output `outputs` (Y).\n- Calculate the loss using `label` and `outputs` using criterion `nn.CrossEntropyLoss()` \n- With the loss, we can now find the gradients and perform Backprogation using `loss.backward()` and optimizer `optim.Adam()` with defined LR\n- `optim.lr_scheduler.StepLR()` will update the LR based on the parameters defined.\n- Find the epoch accuracy and epoch loss for both training and validation phase.\n- Additionally we find validation kappa score. Kappa score is a trusted metric for evaluating Medical AI Models. You can learn more about [here](https://towardsdatascience.com/interpretation-of-kappa-values-2acd1ca7b18f)\n\n**Note : Important step while training in PyTorch is `optimizer.zero_grad()` because PyTorch haves caches of gradients in the memory, we have to zero that out while each step.**\n\n\n#### Early Stopping\nEarly Stopping is a method of choosing the best performed model with given conditions. Generally the conditions would be Best Validation Accuracy or Lowest Validation Loss. For this model, I choose the model with lowest validation loss for better reliable results from the model. Because more medical model is gonna diagnose a person, we dont want a false positive or false negative. \n\nTo do that we will check for lowest validation loss at every epoch and save the model parameters weights into a object and finally load the weights to the model using `model.load_state_dict()`.","metadata":{}},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = np.inf\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            current_loss = 0.0\n            current_corrects = 0\n            current_kappa = 0\n            val_kappa = list()\n\n            for inputs, labels in tqdm.tqdm(dataloaders[phase], desc=phase, leave=False):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # We need to zero the gradients in the Cache.\n                optimizer.zero_grad()\n\n                # Time to carry out the forward training poss\n                # We only need to log the loss stats if we are in training phase\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                if phase == 'train':\n                    scheduler.step()\n\n                # We want variables to hold the loss statistics\n                current_loss += loss.item() * inputs.size(0)\n                current_corrects += torch.sum(preds == labels.data)\n                val_kappa.append(cohen_kappa_score(preds.cpu().numpy(), labels.data.cpu().numpy()))\n            epoch_loss = current_loss / data_sizes[phase]\n            epoch_acc = current_corrects.double() / data_sizes[phase]\n            if phase == 'val':\n                epoch_kappa = np.mean(val_kappa)\n                print('{} Loss: {:.4f} | {} Accuracy: {:.4f} | Kappa Score: {:.4f}'.format(\n                    phase, epoch_loss, phase, epoch_acc, epoch_kappa))\n            else:\n                print('{} Loss: {:.4f} | {} Accuracy: {:.4f}'.format(\n                    phase, epoch_loss, phase, epoch_acc))\n\n            # EARLY STOPPING\n            if phase == 'val' and epoch_loss < best_loss:\n                print('Val loss Decreased from {:.4f} to {:.4f} \\nSaving Weights... '.format(best_loss, epoch_loss))\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_since = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_since // 60, time_since % 60))\n    print('Best val loss: {:.4f}'.format(best_loss))\n\n    # Now we'll load in the best model weights and return it\n    model.load_state_dict(best_model_wts)\n    return model","metadata":{"id":"crbKfZOn5uSx","execution":{"iopub.status.busy":"2021-10-07T06:06:36.213424Z","iopub.execute_input":"2021-10-07T06:06:36.214044Z","iopub.status.idle":"2021-10-07T06:06:36.236645Z","shell.execute_reply.started":"2021-10-07T06:06:36.213996Z","shell.execute_reply":"2021-10-07T06:06:36.234868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define a function which would give image visualisations along with the predicted label to see if the model is really outputing relevant answers.","metadata":{}},{"cell_type":"code","source":"def visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_handeled = 0\n    ax = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n    \n            for j in range(inputs.size()[0]):\n                images_handeled += 1    \n                ax = plt.subplot(num_images//2, 2, images_handeled)\n                ax.axis('off')\n                ax.set_title('Actual: {} predicted: {}'.format(class_names[labels[j].item()],class_names[preds[j]]))\n                imshow(inputs.cpu().data[j], (5,5))\n\n                if images_handeled == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)","metadata":{"id":"iADbMi3tPo2j","execution":{"iopub.status.busy":"2021-10-07T06:06:36.238745Z","iopub.execute_input":"2021-10-07T06:06:36.239708Z","iopub.status.idle":"2021-10-07T06:06:36.256813Z","shell.execute_reply.started":"2021-10-07T06:06:36.239662Z","shell.execute_reply":"2021-10-07T06:06:36.255569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am have run with setting of 10 Epochs due to Kaggle, feel free to tweak up the no.of epochs to get even better model","metadata":{}},{"cell_type":"code","source":"base_model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10)\n","metadata":{"id":"bPW9rQGJPtWY","outputId":"5800afdd-59eb-4903-dc4a-97b35752b1f8","execution":{"iopub.status.busy":"2021-10-07T06:06:36.258779Z","iopub.execute_input":"2021-10-07T06:06:36.259321Z","iopub.status.idle":"2021-10-07T06:14:46.444238Z","shell.execute_reply.started":"2021-10-07T06:06:36.25926Z","shell.execute_reply":"2021-10-07T06:14:46.443038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_model(base_model)\nplt.show()","metadata":{"id":"u97Dv2eTrsim","outputId":"d500d6be-1a7d-4469-be4d-7baeed1d2198","execution":{"iopub.status.busy":"2021-10-07T06:14:46.446807Z","iopub.execute_input":"2021-10-07T06:14:46.44735Z","iopub.status.idle":"2021-10-07T06:14:48.584195Z","shell.execute_reply.started":"2021-10-07T06:14:46.447302Z","shell.execute_reply":"2021-10-07T06:14:48.583418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation\nThat's it, we have trained our model with great accuracy and minimum validation loss, that means it should do well with the images, it haven't seen. So we test the model with images it haven't seen before. This process is called Model Evaluation.\n\nI use a dataset collected and published in Kaggle by @Tawsifur Rahman - [COVID-19 Radiography Database](https://www.kaggle.com/tawsifurrahman/covid19-radiography-database)","metadata":{}},{"cell_type":"code","source":"os.makedirs('./COVID19-DATASET/test/covid19')\nos.makedirs('./COVID19-DATASET/test/normal')","metadata":{"id":"OwXMP4LhLpGj","execution":{"iopub.status.busy":"2021-10-07T06:14:48.586159Z","iopub.execute_input":"2021-10-07T06:14:48.586857Z","iopub.status.idle":"2021-10-07T06:14:48.593265Z","shell.execute_reply.started":"2021-10-07T06:14:48.586777Z","shell.execute_reply":"2021-10-07T06:14:48.592055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I take 100 Images from each class for testing from his repository.","metadata":{}},{"cell_type":"code","source":"COVID_TEST = '../input/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID'\nNORMAL_TEST = '../input/covid19-radiography-database/COVID-19_Radiography_Dataset/Normal'\n\nfor image in os.listdir(COVID_TEST)[:100]:\n    shutil.copy(os.path.join(COVID_TEST, image), os.path.join('./COVID19-DATASET/test/covid19', image))\nfor image in os.listdir(NORMAL_TEST)[:100]:\n    shutil.copy(os.path.join(NORMAL_TEST, image), os.path.join('./COVID19-DATASET/test/normal', image))","metadata":{"id":"lEWvPBn3MAsJ","execution":{"iopub.status.busy":"2021-10-07T06:24:45.708222Z","iopub.execute_input":"2021-10-07T06:24:45.708607Z","iopub.status.idle":"2021-10-07T06:24:45.89035Z","shell.execute_reply.started":"2021-10-07T06:24:45.708564Z","shell.execute_reply":"2021-10-07T06:24:45.889439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_DATA_PATH = './COVID19-DATASET/test/'\n\ntest_transforms = transforms.Compose([\n                                      transforms.Resize((150,150)),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=mean_nums, std=std_nums)\n])\n\n\ntest_image = datasets.ImageFolder(TEST_DATA_PATH, transform=test_transforms)\n\ntestloader = torch.utils.data.DataLoader(test_image, batch_size=1)","metadata":{"id":"pYAcMYROHhiv","execution":{"iopub.status.busy":"2021-10-07T06:24:45.895575Z","iopub.execute_input":"2021-10-07T06:24:45.895909Z","iopub.status.idle":"2021-10-07T06:24:45.90734Z","shell.execute_reply.started":"2021-10-07T06:24:45.89588Z","shell.execute_reply":"2021-10-07T06:24:45.906311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_list = []\ny_true_list = []\nwith torch.no_grad():\n    for x_batch, y_batch in tqdm.tqdm(testloader, leave=False):\n        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n        y_test_pred = base_model(x_batch)\n        y_test_pred = torch.log_softmax(y_test_pred, dim=1)\n        _, y_pred_tag = torch.max(y_test_pred, dim = 1)\n        y_pred_list.append(y_pred_tag.cpu().numpy())\n        y_true_list.append(y_batch.cpu().numpy())","metadata":{"id":"Ff-vTLmcS2VG","outputId":"957c9dd3-3c2d-46d0-ac5a-4f31a7f95ada","execution":{"iopub.status.busy":"2021-10-07T06:24:45.911518Z","iopub.execute_input":"2021-10-07T06:24:45.911828Z","iopub.status.idle":"2021-10-07T06:24:52.78555Z","shell.execute_reply.started":"2021-10-07T06:24:45.911798Z","shell.execute_reply":"2021-10-07T06:24:52.784168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_list = [i[0] for i in y_pred_list]\ny_true_list = [i[0] for i in y_true_list]","metadata":{"id":"tL9SAsUwLQ5x","execution":{"iopub.status.busy":"2021-10-07T06:24:52.787735Z","iopub.execute_input":"2021-10-07T06:24:52.788183Z","iopub.status.idle":"2021-10-07T06:24:52.795834Z","shell.execute_reply.started":"2021-10-07T06:24:52.788137Z","shell.execute_reply":"2021-10-07T06:24:52.794426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_true_list, y_pred_list))","metadata":{"id":"whvypZEYNGkA","outputId":"0090786e-4d42-46cb-aa9d-c50f4906bb37","execution":{"iopub.status.busy":"2021-10-07T06:24:52.799939Z","iopub.execute_input":"2021-10-07T06:24:52.800783Z","iopub.status.idle":"2021-10-07T06:24:52.818338Z","shell.execute_reply.started":"2021-10-07T06:24:52.800734Z","shell.execute_reply":"2021-10-07T06:24:52.816873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport itertools\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","metadata":{"id":"NUBFOsyAYRly","execution":{"iopub.status.busy":"2021-10-07T06:24:52.820551Z","iopub.execute_input":"2021-10-07T06:24:52.820992Z","iopub.status.idle":"2021-10-07T06:24:52.842453Z","shell.execute_reply.started":"2021-10-07T06:24:52.82095Z","shell.execute_reply":"2021-10-07T06:24:52.840928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm =  confusion_matrix(y_true_list, y_pred_list)\n\nplot_confusion_matrix(cm = cm, \n                      normalize    = False,\n                      target_names = ['covid19','normal'],\n                      title        = \"Confusion Matrix\")","metadata":{"id":"ZyptU5r-ZNiL","outputId":"7000543c-8f87-4844-9d80-be40651c183f","execution":{"iopub.status.busy":"2021-10-07T06:24:52.84439Z","iopub.execute_input":"2021-10-07T06:24:52.845076Z","iopub.status.idle":"2021-10-07T06:24:53.067403Z","shell.execute_reply.started":"2021-10-07T06:24:52.845022Z","shell.execute_reply":"2021-10-07T06:24:53.066039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(base_model.state_dict(), './best_model.pth')","metadata":{"execution":{"iopub.status.busy":"2021-10-07T06:24:53.069113Z","iopub.execute_input":"2021-10-07T06:24:53.06953Z","iopub.status.idle":"2021-10-07T06:24:53.158122Z","shell.execute_reply.started":"2021-10-07T06:24:53.069489Z","shell.execute_reply":"2021-10-07T06:24:53.157094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference \nNow that you trained and tested your model, you gotta a very good model on your hand. Now we can take this to production with inference code.\n\nAn inference code is the one which is used to predict a single input image so that it can be integrated into an web app or smartphone application. Usually the inferences are done in CPU because of the hardware constraints in deployment.","metadata":{}},{"cell_type":"code","source":"# Loading the model pre-trained weights from saved file\ninf_model = CNN_Model(pretrained=False)\ninf_model.to(torch.device('cpu'))\ninf_model.load_state_dict(torch.load('./best_model.pth', map_location='cpu'))\ninf_model.eval()\nprint('Inference Model Loaded on CPU')","metadata":{"id":"z8z6gipPNHPQ","outputId":"89989076-7839-4b03-fe5d-9d53c7c5d395","execution":{"iopub.status.busy":"2021-10-07T06:24:53.159816Z","iopub.execute_input":"2021-10-07T06:24:53.160323Z","iopub.status.idle":"2021-10-07T06:24:53.543262Z","shell.execute_reply.started":"2021-10-07T06:24:53.160275Z","shell.execute_reply":"2021-10-07T06:24:53.542108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom torch.autograd import Variable","metadata":{"id":"obv_9qDTOcKd","execution":{"iopub.status.busy":"2021-10-07T06:24:53.544978Z","iopub.execute_input":"2021-10-07T06:24:53.545791Z","iopub.status.idle":"2021-10-07T06:24:53.551851Z","shell.execute_reply.started":"2021-10-07T06:24:53.545743Z","shell.execute_reply":"2021-10-07T06:24:53.550345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Inference function\nimport cv2\nfrom keras.preprocessing.image import array_to_img, img_to_array\n\ndef predict(img_path):\n    image = cv2.imread(img_path)\n    image = cv2.resize(image, (150,150))\n    if image.shape[2] ==1:\n                image_tensor = np.dstack([image, image, image])\n    print(image.shape)\n#     image = img_to_array(image)\n#     image_tensor = test_transforms(image)\n#     image_tensor = image_tensor.unsqueeze_(0)\n    input = Variable(image_tensor)\n    # print(input.shape)\n    input = input.to(torch.device('cpu'))\n    out = inf_model(input)\n    _, preds = torch.max(out, 1)\n    idx = preds.cpu().numpy()[0]\n    pred_class = class_names[idx]\n    score = out[0][0].item()\n    plt.imshow(np.array(image))\n    print(\"Predicted: {}\".format(pred_class))","metadata":{"id":"F-9oMRohNZpC","execution":{"iopub.status.busy":"2021-10-07T06:44:20.25434Z","iopub.execute_input":"2021-10-07T06:44:20.25484Z","iopub.status.idle":"2021-10-07T06:44:20.266991Z","shell.execute_reply.started":"2021-10-07T06:44:20.254808Z","shell.execute_reply":"2021-10-07T06:44:20.265581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls './COVID19-DATASET/test/covid19/'","metadata":{"execution":{"iopub.status.busy":"2021-10-07T06:28:41.574407Z","iopub.execute_input":"2021-10-07T06:28:41.5751Z","iopub.status.idle":"2021-10-07T06:28:42.366288Z","shell.execute_reply.started":"2021-10-07T06:28:41.575055Z","shell.execute_reply":"2021-10-07T06:28:42.364828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict('./COVID19-DATASET/test/covid19/COVID-1007.png') #Sample Image Path","metadata":{"id":"8QP4c2KrP503","outputId":"d5f24088-75fa-4290-ab1e-30c46c65576e","execution":{"iopub.status.busy":"2021-10-07T06:44:23.450355Z","iopub.execute_input":"2021-10-07T06:44:23.450823Z","iopub.status.idle":"2021-10-07T06:44:23.493688Z","shell.execute_reply.started":"2021-10-07T06:44:23.450759Z","shell.execute_reply":"2021-10-07T06:44:23.491141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Young Anakin Tinkering with C-3PO](https://qph.fs.quoracdn.net/main-qimg-4247d864f62ecdb071c8f259d15947af.webp)\n\n\nWe have created the eye of our own C-3PO for Health Care. Next thing is develop ability to sense sentiment through listening. Would like to learn how to do that?\n\nPlease check out my notebook on [Sentiment Classification using LSTM](https://www.kaggle.com/arunrk7/nlp-beginner-text-classification-using-lstm)","metadata":{}},{"cell_type":"markdown","source":"Hurrah! Let's celebrate our victory over creating a AI which can help us diagnose CoronaVirus Patients. \n\n> ‚ö†Ô∏è But I would never claim this model to diagnose actual Patients just yet. Since we have trained this model on a very small data. We can never make assumptions or claim the model is the new Baseline Performer and it is ready to deploy for diagnosis in real world. Unless you are a Medical Expert and get the Medical Council of your country approve this. You are not allowed to proclaim your model as potential diagnosis tool.\n\nI wrote this notebook to give a glimpse over the Image Classification using Deep Learning in PyTorch. I hope you have learnt something from this notebook. Best wishes on your future journey of Creating AI.\n\nI would love to suggest you some resources to learn more about this incredible field.\n- deeplearning.ai's courses on [Convolutional Neural Network](https://www.coursera.org/learn/convolutional-neural-networks) and [AI on Medicine](https://www.deeplearning.ai/ai-for-medicine/)\n- PyTorch [Tutorials](https://pytorch.org/tutorials/) some of them are very intriguing \n- MLMED Chester Xray Diagnosis Assistant on [Github](https://github.com/mlmed/dl-web-xray)\n\n<font color='yellow'> May the force be with all of us!!! Survive this pandemic we will </font>\n\n\n\n<font size=5 color='red'> If you find this notebook helpful, please do <strong>UPVOTE</strong>. It would cheer me up and write more.</font>","metadata":{}}]}